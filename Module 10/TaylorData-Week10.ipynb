{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cristian Taylor\n",
    "## 8/7/2024\n",
    "## DSC320 Math for Data Science \n",
    "## Week 10 - __Probabilities and Entropy__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.15000000000000002, 0.2969387755102041)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given probabilities\n",
    "P_A = 0.3\n",
    "P_B = 0.4\n",
    "P_C = 0.3\n",
    "\n",
    "P_D_given_A = 0.01\n",
    "P_D_given_B = 0.02\n",
    "P_D_given_C = 0.03\n",
    "\n",
    "P_non_D_given_A = 0.99\n",
    "P_non_D_given_B = 0.98\n",
    "P_non_D_given_C = 0.97\n",
    "\n",
    "# Total probability of defectiveness\n",
    "P_D = (P_D_given_A * P_A) + (P_D_given_B * P_B) + (P_D_given_C * P_C)\n",
    "\n",
    "# Total probability of non-defectiveness\n",
    "P_non_D = (P_non_D_given_A * P_A) + (P_non_D_given_B * P_B) + (P_non_D_given_C * P_C)\n",
    "\n",
    "# Bayes' Theorem for P(A | D)\n",
    "P_A_given_D = (P_D_given_A * P_A) / P_D\n",
    "\n",
    "# Bayes' Theorem for P(C | non-D)\n",
    "P_C_given_non_D = (P_non_D_given_C * P_C) / P_non_D\n",
    "\n",
    "P_A_given_D, P_C_given_non_D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.321928094887362"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(probabilities):\n",
    "    \"\"\"Calculate the entropy of a probability distribution.\"\"\"\n",
    "    return -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "\n",
    "# Test the function with some example probabilities\n",
    "entropy([0.2, 0.2, 0.2, 0.2, 0.2])  # Uniform distribution, should give maximum entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.321928094887362, 2.046439344671015)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_X = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "P_Y = [0.1, 0.4, 0.1, 0.3, 0.1]\n",
    "\n",
    "H_X = entropy(P_X)\n",
    "H_Y = entropy(P_Y)\n",
    "\n",
    "H_X, H_Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X's entropy was higher due to having a uniform distribution and having no variance in the P(X-x) column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
